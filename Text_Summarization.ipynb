{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-Summarization.ipynb",
      "provenance": [],
      "mount_file_id": "https://gist.github.com/haidour18/a45f2f7830837abac357c6a4d879a628#file-text-summarization-ipynb",
      "authorship_tag": "ABX9TyOwAcnFA8Xnx1AJRRfJDmUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haidour18/NLP/blob/main/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J667xMzHUg8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b678ac92-cc81-457a-fc1d-6b6eecc977c8"
      },
      "source": [
        "#install dependecies\n",
        "!pip install kaggle\n",
        "!pip install -q keras\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install Attention \n",
        "!pip install keras-self-attention\n",
        "!pip install keras-attention\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: Attention in /usr/local/lib/python3.6/dist-packages (3.0)\n",
            "Requirement already satisfied: keras>=2.3.1 in /usr/local/lib/python3.6/dist-packages (from Attention) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.6/dist-packages (from Attention) (1.19.5)\n",
            "Requirement already satisfied: tensorflow>=2.1 in /usr/local/lib/python3.6/dist-packages (from Attention) (2.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.3.1->Attention) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.3.1->Attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.3.1->Attention) (2.10.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (0.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (1.32.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (2.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1->Attention) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1->Attention) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->Attention) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->Attention) (1.25.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->Attention) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->Attention) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->Attention) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1->Attention) (3.3.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->Attention) (4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->Attention) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->Attention) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->Attention) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->Attention) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->Attention) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->Attention) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1->Attention) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->Attention) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1->Attention) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1->Attention) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1->Attention) (3.4.0)\n",
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.49.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n",
            "Collecting keras-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/e9/1373dff1b2dd7cb45208d224c84d7a2bddcf0519f01d98f07d8ce15dc4be/keras_attention-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-attention) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->keras-attention) (1.15.0)\n",
            "Installing collected packages: keras-attention\n",
            "Successfully installed keras-attention-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb9UZLqUgKQk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH-LUDG9c5PP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18eaeeb7-a189-4df3-fca2-81207226ccdd"
      },
      "source": [
        "from google.colab import  drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "nZHv2nkXxkF7",
        "outputId": "71bb4c88-27d2-454d-ad5d-82d15e79eb2d"
      },
      "source": [
        "from google.colab import files \n",
        "files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-479994da-1900-48c1-8fa0-5930eed22b73\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-479994da-1900-48c1-8fa0-5930eed22b73\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"asmaahaidour\",\"key\":\"9558ae9a1dcd81c5213ff117d35c9658\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGx_gCt5deEb"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "#change permissions \n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nru6z47Se8Me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e3f972-2c07-4c8d-ca2b-87a0b9f7909a"
      },
      "source": [
        "!kaggle datasets download -d hsankesara/medium-articles"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading medium-articles.zip to /content\n",
            "\r  0% 0.00/1.34M [00:00<?, ?B/s]\n",
            "\r100% 1.34M/1.34M [00:00<00:00, 93.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkKvUlGkxrzd",
        "outputId": "d35dec2a-7ef8-43b3-bde3-82b79abdff0b"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7V_i-U3x8xM"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os, glob , csv\n",
        "import time, joblib\n",
        "import pandas as pd\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import seaborn as sns"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HiO4JlJyVBR"
      },
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "from gensim.summarization import keywords"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0TbEQxyylo7",
        "outputId": "8b32d69d-23d0-4c0e-c3d4-b2c210be4b36"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from nltk import download\n",
        "download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hj8MshEY0vRK",
        "outputId": "d65f377a-fbee-4cb8-f38e-a3a7b41704b9"
      },
      "source": [
        "#Let’s see some top 5 headline and text, and print our data shape.\n",
        "\n",
        "reviews = pd.read_csv(\"articles.csv\")\n",
        "print(reviews.shape)\n",
        "reviews.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(337, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>claps</th>\n",
              "      <th>reading_time</th>\n",
              "      <th>link</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Justin Lee</td>\n",
              "      <td>8.3K</td>\n",
              "      <td>11</td>\n",
              "      <td>https://medium.com/swlh/chatbots-were-the-next-big-thing-what-happened-5fc49dd6fa61?source=---------0----------------</td>\n",
              "      <td>Chatbots were the next big thing: what happened? – The Startup – Medium</td>\n",
              "      <td>Oh, how the headlines blared:\\nChatbots were The Next Big Thing.\\nOur hopes were sky high. Bright-eyed and bushy-tailed, the industry was ripe for a new era of innovation: it was time to start soc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Conor Dewey</td>\n",
              "      <td>1.4K</td>\n",
              "      <td>7</td>\n",
              "      <td>https://towardsdatascience.com/python-for-data-science-8-concepts-you-may-have-forgotten-i-did-825966908393?source=---------1----------------</td>\n",
              "      <td>Python for Data Science: 8 Concepts You May Have Forgotten</td>\n",
              "      <td>If you’ve ever found yourself looking up the same question, concept, or syntax over and over again when programming, you’re not alone.\\nI find myself doing this constantly.\\nWhile it’s not unnatur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>William Koehrsen</td>\n",
              "      <td>2.8K</td>\n",
              "      <td>11</td>\n",
              "      <td>https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219?source=---------2----------------</td>\n",
              "      <td>Automated Feature Engineering in Python – Towards Data Science</td>\n",
              "      <td>Machine learning is increasingly moving from hand-designed models to automatically optimized pipelines using tools such as H20, TPOT, and auto-sklearn. These libraries, along with methods such as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gant Laborde</td>\n",
              "      <td>1.3K</td>\n",
              "      <td>7</td>\n",
              "      <td>https://medium.freecodecamp.org/machine-learning-how-to-go-from-zero-to-hero-40e26f8aa6da?source=---------3----------------</td>\n",
              "      <td>Machine Learning: how to go from Zero to Hero – freeCodeCamp</td>\n",
              "      <td>If your understanding of A.I. and Machine Learning is a big question mark, then this is the blog post for you. Here, I gradually increase your AwesomenessicityTM by gluing inspirational videos tog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Emmanuel Ameisen</td>\n",
              "      <td>935</td>\n",
              "      <td>11</td>\n",
              "      <td>https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8?source=---------4----------------</td>\n",
              "      <td>Reinforcement Learning from scratch – Insight Data</td>\n",
              "      <td>Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program.\\nAre y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             author  ...                                                                                                                                                                                                     text\n",
              "0        Justin Lee  ...  Oh, how the headlines blared:\\nChatbots were The Next Big Thing.\\nOur hopes were sky high. Bright-eyed and bushy-tailed, the industry was ripe for a new era of innovation: it was time to start soc...\n",
              "1       Conor Dewey  ...  If you’ve ever found yourself looking up the same question, concept, or syntax over and over again when programming, you’re not alone.\\nI find myself doing this constantly.\\nWhile it’s not unnatur...\n",
              "2  William Koehrsen  ...  Machine learning is increasingly moving from hand-designed models to automatically optimized pipelines using tools such as H20, TPOT, and auto-sklearn. These libraries, along with methods such as ...\n",
              "3      Gant Laborde  ...  If your understanding of A.I. and Machine Learning is a big question mark, then this is the blog post for you. Here, I gradually increase your AwesomenessicityTM by gluing inspirational videos tog...\n",
              "4  Emmanuel Ameisen  ...  Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program.\\nAre y...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQDgb25m1LUD",
        "outputId": "e8eeade6-f3c6-4a4b-8f2f-b5d49c19016f"
      },
      "source": [
        "#Check Null values \n",
        "reviews.isnull().sum()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author          0\n",
              "claps           0\n",
              "reading_time    0\n",
              "link            0\n",
              "title           0\n",
              "text            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClkU3WU01as6"
      },
      "source": [
        "#Cleaning the text \n",
        "CURRENCIES = {\n",
        "    \"$\": \"USD\", \"zł\": \"PLN\", \"£\": \"GBP\", \"¥\": \"JPY\", \"฿\": \"THB\", \"₡\": \"CRC\", \"₦\": \"NGN\",\"₩\": \"KRW\",\n",
        "    \"₪\": \"ILS\", \"₫\": \"VND\", \"€\": \"EUR\", \"₱\": \"PHP\", \"₲\": \"PYG\", \"₴\": \"UAH\", \"₹\": \"INR\",}\n",
        "CURRENCY_REGEX = re.compile(\n",
        "    \"({})+\".format(\"|\".join(re.escape(c) for c in CURRENCIES.keys())))\n",
        "\n",
        "EMAIL_REGEX = re.compile(\n",
        "    r\"(?:^|(?<=[^\\w@.)]))([\\w+-](\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(\\.([a-z]{2,})){1,3}(?:$|(?=\\b))\",\n",
        "    flags=re.IGNORECASE | re.UNICODE,)\n",
        "\n",
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions =          {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\", \"i've\": \"i have\"}\n",
        "def clean_text(text, remove_stopwords = True):\n",
        "    \n",
        "    text = text.lower()\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "        \n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = EMAIL_REGEX.sub(' ',text)\n",
        "    text = CURRENCY_REGEX.sub(' ',text)\n",
        "    text = ' '.join([contractions[t] if t in contractions else t for t in text.split(\" \")])    \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r\"'s\\b\",\"\", text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    \n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfxm9HgY2rh0",
        "outputId": "ef969c31-2e07-40a2-83d8-8beaa1eff6e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cleaned_headlines = []\n",
        "cleaned_text = []\n",
        "\n",
        "for title in reviews['title']:\n",
        "    cleaned_headlines.append(clean_text(title, remove_stopwords=False))\n",
        "print(\"Headlines are complete.\")\n",
        "\n",
        "for text in reviews['text']:\n",
        "    cleaned_text.append(clean_text(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Headlines are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFukYF-r251x",
        "outputId": "5b26630d-75b3-4280-a487-cc1c3fd4fecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(4):\n",
        "    print(\"Review: \",i+1) # You can change it by \"Review\" to \"Headline\"\n",
        "    print(cleaned_headlines[i])\n",
        "    print('-'*80)\n",
        "    print(cleaned_text[i])\n",
        "    print()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review:  1\n",
            "chatbots were the next big thing  what happened  – the startup – medium\n",
            "--------------------------------------------------------------------------------\n",
            "oh headlines blared chatbots next big thing hopes sky high bright eyed bushy tailed industry ripe new era innovation time start socializing machines wouldn’t road signs pointed towards insane success mobile world congress 2017 chatbots main headliners conference organizers cited ‘overwhelming acceptance event inevitable shift focus brands corporates chatbots’ fact significant question around chatbots would monopolize field whether chatbots would take first place one year answer question isn’t even ecosystem platform dominate chatbots weren’t first technological development talked grandiose terms slump spectacularly age old hype cycle unfolded familiar fashion expectations built built kind fizzled predicted paradim shift didn’t materialize apps tellingly still alive well look back breathless optimism turn slightly baffled “is chatbot revolution promised ” digit’s ethan bloch sums general consensus according dave feldman vice president product design heap chatbots didn’t take one difficult problem fail took several failed bots interface users different ways big divide text vs speech beginning computer interfaces written word users type commands manually machine get anything done graphical user interfaces guis came along saved day became entranced windows mouse clicks icons hey eventually got color meanwhile bunch research scientists busily developing natural language nl interfaces databases instead learn arcane database query language another bunch scientists developing speech processing software could speak computer rather type turned whole lot difficult anyone originally realised next item agenda holding two way dialog machine here’s example dialog dating back 1990s vcr setup system pretty cool right system takes turns collaborative way smart job figuring user wants carefully crafted deal conversations involving vcrs could operate within strict limitations modern day bots whether use typed spoken input face challenges also work efficient scalable way variety platforms basically we’re still trying achieve innovations 30 years ago here’s think we’re going wrong oversized assumption apps ‘over’ would replaced bots pitting two disparate concepts one another instead seeing separate entities designed serve different purposes discouraged bot development might remember similar war cry apps first came onto scene ten years ago remember apps replaced internet it’s said new product service needs two following better cheaper faster chatbots cheaper faster apps — yet least whether they’re ‘better’ subjective think it’s fair say today’s best bot isn’t comparable today’s best app plus nobody thinks using lyft complicated it’s hard order food buy dress app complicated trying complete tasks bot — bot fail great bot useful average app comes rich sophisticated multi layered apps there’s competition that’s machines let us access vast complex information systems early graphical information systems revolutionary leap forward helping us locate systems modern day apps benefit decades research experimentation would throw away swap word ‘replace’ ‘extend’ things get much interesting today’s successful bot experiences take hybrid approach incorporating chat broader strategy encompasses traditional elements next wave multimodal apps say want like siri get back information map text even spoken response another problematic aspect sweeping nature hype tends bypass essential questions like plenty companies bots aren’t right solution past two years littered cases bots blindly applied problems aren’t needed building bot sake letting loose hoping best never end well vast majority bots built using decision tree logic bot’s canned response relies spotting specific keywords user input advantage approach it’s pretty easy list cases designed cover that’s precisely disadvantage that’s bots purely reflection capability fastidiousness patience person created many user needs inputs able anticipate problems arise life refuses fit boxes according recent reports 70 100 000 bots facebook messenger failing fulfil simple user requests partly result developers failing narrow bot one strong area focus building growthbot decided make specific sales marketers ‘all rounder’ despite temptation get overexcited potential capabilties remember bot one thing well infinitely helpful bot multiple things poorly competent developer build basic bot minutes — one hold conversation that’s another story despite constant hype around ai we’re still long way achieving anything remotely human like ideal world technology known nlp natural language processing allow chatbot understand messages receives nlp emerging research labs much infancy platforms provide bit nlp even best toddler level capacity example think siri understanding words meaning matt asay outlines results another issue failure capture attention creativity developers conversations complex they’re linear topics spin around take random turns restart abruptly finish today’s rule based dialogue systems brittle deal kind unpredictability statistical approaches using machine learning limited level ai required human like conversation isn’t available yet meantime high quality examples trailblazing bots lead way dave feldman remarked upon time way interact computers typing arcane commands terminal visual interfaces using windows icons mouse revolution manipulate information there’s reasons computing moved text based graphical user interfaces guis input side it’s easier faster click type tapping selecting obviously preferable typing whole sentence even predictive often error prone text output side old adage picture worth thousand words usually true love optical displays information highly visual creatures it’s accident kids love touch screens pioneers dreamt graphical interface inspired cognitive psychology study brain deals communication conversational uis meant replicate way humans prefer communicate end requiring extra cognitive effort essentially we’re swapping something simple complex alternative sure concepts express using language “show ways getting museum give 2000 steps don’t take longer 35 minutes” tasks carried efficiently intuitively guis conversational ui aiming human dimension business interactions makes sense there’s one thing that’s broken sales marketing it’s lack humanity brands hide behind ticket numbers feedback forms reply emails automated responses gated ‘contact us’ forms facebook’s goal bots pass called turing test meaning can’t tell whether talking bot human bot isn’t human never conversation encompasses much text humans read lines leverage contextual information understand double layers like sarcasm bots quickly forget they’re talking meaning it’s bit like conversing someone little short term memory hubspot team pinpointed people aren’t easily fooled pretending bot human guaranteed diminish returns mention fact you’re lying users even rare bots powered state art nlp excel processing producing content fall short comparison here’s thing conversational uis built replicate way humans prefer communicate — humans humans prefer interact machines necessarily end day amount witty quips human like mannerisms save bot conversational failure way early adopters weren’t entirely wrong people yelling google home play favorite song ordering pizza domino’s bot getting makeup tips sephora terms consumer response developer involvement chatbots haven’t lived hype generated circa 2015 16 even close computers good computers searching data crunching numbers analyzing opinions condensing information computers aren’t good understanding human emotion state nlp means still don’t ‘get’ we’re asking never mind feel that’s it’s still impossible imagine effective customer support sales marketing without essential human touch empathy emotional intelligence bots continue help us automated repetitive low level tasks queries cogs larger complex system disservice expecting much soon that’s whole story yes industry massively overestimated initial impact chatbots would emphasis initial bill gates said hype that’s good thing start examining middle grounded grey area instead hyper inflated frantic black white zone believe we’re beginning explosive growth sense anti climax completely normal transformational technology messaging continue gain traction chatbots aren’t going away nlp ai becoming sophisticated every day developers apps platforms continue experiment heavily invest conversational marketing can’t wait see happens next quick cheer standing ovation clap show much enjoyed story head growth growthbot messaging conversational strategy hubspot medium largest publication makers subscribe receive top stories →\n",
            "\n",
            "Review:  2\n",
            "python for data science  8 concepts you may have forgotten\n",
            "--------------------------------------------------------------------------------\n",
            "you’ve ever found looking question concept syntax programming you’re alone find constantly it’s unnatural look things stackoverflow resources slow good bit raise questions complete understanding language live world seemingly infinite amount accessible free resources looming one search away times however blessing curse managed effectively reliance resources build poor habits set back long term personally find pulling code similar discussion threads several times rather taking time learn solidify concept reproduce code next time approach lazy may path least resistance short term ultimately hurt growth productivity ability recall syntax cough interviews line recently i’ve working online data science course titled python data science machine learning udemy oh god sound like guy youtube early lectures series reminded concepts syntax consistently overlook performing data analysis python interest solidifying understanding concepts saving guys couple stackoverflow searches here’s stuff i’m always forgetting working python numpy pandas i’ve included short description example however benefit also include links videos resources explore concept depth well writing loop every time need define sort list tedious luckily python built way address problem one line code syntax little hard wrap head around get familiar technique you’ll use fairly often see example would normally go list comprehension loop vs creating list one simple line loops necessary ever get tired creating function function limited use cases lambda functions rescue lambda functions used creating small one time anonymous function objects python basically let create function without creating function basic syntax lambda functions note lambda functions everything regular functions long there’s one expression check simple example upcoming video get better feel power lambda functions grasp lambda functions learning pair map filter functions powerful tool specifically map takes list transforms new list performing sort operation element example goes element maps result times 2 new list note list function simply converts output list type filter function takes list rule much like map however returns subset original list comparing element boolean filtering rule creating quick easy numpy arrays look arange linspace functions one specific purpose appeal instead using range output numpy arrays typically easier work data science arange returns evenly spaced values within given interval along starting stopping point also define step size data type necessary note stopping point ‘cut off’ value included array output linspace similar slight twist linspace returns evenly spaced numbers specified interval given starting stopping point well number values linspace evenly space numpy array especially helpful data visualizations declaring axes plotting may ran dropping column pandas summing values numpy matrix surely point let’s use example dropping column don’t know many times wrote line code actually knew declaring axis probably deduce set axis 1 want deal columns set 0 want rows favorite reasoning atleast remember calling shape attribute pandas dataframe gives us back tuple first value representing number rows second value representing number columns think indexed python rows 0 columns 1 much like declare axis value crazy right you’re familiar sql concepts probably come lot easier anyhow functions essentially ways combine dataframes specific ways difficult keep track best use time let’s review concat allows user append one dataframes either next depending define axis merge combines multiple dataframes specific common columns serve primary key join much like merge combines two dataframes however joins based indices rather specified column check excellent pandas documentation specific syntax concrete examples well special cases may run think apply map function made pandas dataframes specifically series you’re familiar series pretty similar numpy arrays part apply sends function every element along column row depending specify might imagine useful especially formatting manipulating values across whole dataframe column without loop last certainly least pivot tables you’re familiar microsoft excel you’ve probably heard pivot tables respect pandas built pivot table function creates spreadsheet style pivot table dataframe note levels pivot table stored multiindex objects index columns resulting dataframe that’s hope couple overviews effectively jogged memory regarding important yet somewhat tricky methods functions concepts frequently encounter using python data science personally know even act writing trying explain simple terms helped ton you’re interested receiving weekly rundown interesting articles resources focused data science machine learning artificial intelligence subscribe self driven data science using form enjoyed post feel free hit clap button you’re interested posts come make sure follow medium link — i’ll writing shipping every day month part 30 day challenge article originally published conordewey com quick cheer standing ovation clap show much enjoyed story data scientist writer www conordewey com sharing concepts ideas codes\n",
            "\n",
            "Review:  3\n",
            "automated feature engineering in python – towards data science\n",
            "--------------------------------------------------------------------------------\n",
            "machine learning increasingly moving hand designed models automatically optimized pipelines using tools h20 tpot auto sklearn libraries along methods random search aim simplify model selection tuning parts machine learning finding best model dataset little manual intervention however feature engineering arguably valuable aspect machine learning pipeline remains almost entirely human labor feature engineering also known feature creation process constructing new features existing data train machine learning model step important actual model used machine learning algorithm learns data give creating features relevant task absolutely crucial see excellent paper “a useful things know machine learning” typically feature engineering drawn manual process relying domain knowledge intuition data manipulation process extremely tedious final features limited human subjectivity time automated feature engineering aims help data scientist automatically creating many candidate features dataset best selected used training article walk example using automated feature engineering featuretools python library use example dataset show basics stay tuned future posts using real world data complete code article available github feature engineering means building additional features existing data often spread across multiple related tables feature engineering requires extracting relevant information data getting single table used train machine learning model process constructing features time consuming new feature usually requires several steps build especially using information one table group operations feature creation two categories transformations aggregations let’s look examples see concepts action transformation acts single table thinking terms python table pandas dataframe creating new features one existing columns example table clients create features finding month joined column taking natural log income column transformations use information one table hand aggregations performed across tables use one many relationship group observations calculate statistics example another table information loans clients client may multiple loans calculate statistics average maximum minimum loans client process involves grouping loans table client calculating aggregations merging resulting data client data here’s would python using language pandas operations difficult hundreds variables spread across dozens tables process feasible hand ideally want solution automatically perform transformations aggregations across multiple tables combine resulting data single table although pandas great resource there’s much data manipulation want hand manual feature engineering check excellent python data science handbook fortunately featuretools exactly solution looking open source python library automatically create many features set related tables featuretools based method known “deep feature synthesis” sounds lot imposing actually name comes stacking multiple features uses deep learning deep feature synthesis stacks multiple transformation aggregation operations called feature primitives vocab featuretools create features data spread across many tables like ideas machine learning it’s complex method built foundation simple concepts learning one building block time form good understanding powerful method first let’s take look example data already saw dataset complete collection tables follows machine learning task predicting whether client repay future loan want combine information clients single table tables related client id loan id variables could use series transformations aggregations process hand however shortly see instead use featuretools automate process first two concepts featuretools entities entitysets entity simply table dataframe think pandas entityset collection tables relationships think entityset another python data structure methods attributes create empty entityset featuretools using following add entities entity must index column unique elements value index must appear table index clients dataframe client idbecause client one row dataframe add entity existing index entityset using following syntax loans dataframe also unique index loan id syntax add entityset clients however payments dataframe unique index add entity entityset need pass parameter make index true specify name index also although featuretools automatically infer data type column entity override passing dictionary column types parameter variable types dataframe even though missed integer numeric variable since take 2 discrete values tell featuretools treat categorical variable adding dataframes entityset inspect column types correctly inferred modification specified next need specify tables entityset related best way think relationship two tables analogy parent child one many relationship parent multiple children realm tables parent table one row every parent child table may multiple rows corresponding multiple children parent example dataset clients dataframe parent loans dataframe client one row clients may multiple rows loans likewise loans parent payments loan multiple payments parents linked children shared variable perform aggregations group child table parent variable calculate statistics across children parent formalize relationship featuretools need specify variable links two tables together clients loans table linked via client id variable loans payments linked loan id syntax creating relationship adding entityset shown entityset contains three entities tables relationships link entities together adding entities formalizing relationships entityset complete ready make features quite get deep feature synthesis need understand feature primitives already know calling different names simply basic operations use form new features new features created featuretools using primitives either stacking multiple primitives list feature primitives featuretools also define custom primitives primitives used combined create features make features specified primitives use ft dfs function standing deep feature synthesis pass entityset target entity table want add features selected trans primitives transformations agg primitives aggregations result dataframe new features client made clients target entity example month client joined transformation feature primitive also number aggregation primitives average payment amounts client even though specified feature primitives featuretools created many new features combining stacking primitives complete dataframe 793 columns new features pieces place understand deep feature synthesis dfs fact already performed dfs previous function call deep feature simply feature made stacking multiple primitives dfs name process makes features depth deep feature number primitives required make feature example mean payments payment amount column deep feature depth 1 created using single aggregation feature depth two last loans mean payments payment amount made stacking two aggregations last recent top mean represents average payment size recent loan client stack features depth want practice never gone beyond depth 2 point features difficult interpret encourage anyone interested try “going deeper” manually specify feature primitives instead let featuretools automatically choose features us use ft dfs function call pass feature primitives featuretools built many new features us use process automatically create new features replace data scientist still figure features example goal predict whether client repay loan could look features correlated specified outcome moreover domain knowledge use choose specific feature primitives seed deep feature synthesis candidate features automated feature engineering solved one problem created another many features although it’s difficult say fitting model features important it’s likely relevant task want train model moreover many features lead poor model performance less useful features drown important problem many features known curse dimensionality number features increases dimension data grows becomes difficult model learn mapping features targets fact amount data needed model perform well scales exponentially number features curse dimensionality combated feature reduction also known feature selection process removing irrelevant features take many forms principal component analysis pca selectkbest using feature importances model auto encoding using deep neural networks however feature reduction different topic another article know use featuretools create numerous features many tables minimal effort like many topics machine learning automated feature engineering featuretools complicated concept built simple ideas using concepts entitysets entities relationships featuretools perform deep feature synthesis create new features deep feature synthesis turn stacks feature primitives — aggregations act across one many relationship tables transformations functions applied one columns single table — build new features multiple tables future articles i’ll show use technique real world problem home credit default risk competition currently hosted kaggle stay tuned post meantime read introduction get started competition hope use automated feature engineering aid data science pipeline models good data give automated feature engineering help make feature creation process efficient information featuretools including advanced usage check online documentation see featuretools used practice read work feature labs company behind open source library always welcome feedback constructive criticism reached twitter koehrsen quick cheer standing ovation clap show much enjoyed story data scientist master student data science communicator advocate sharing concepts ideas codes\n",
            "\n",
            "Review:  4\n",
            "machine learning  how to go from zero to hero – freecodecamp\n",
            "--------------------------------------------------------------------------------\n",
            "understanding machine learning big question mark blog post gradually increase awesomenessicitytm gluing inspirational videos together friendly text sit relax videos take time don’t inspire continue next section fair enough however find bottom article you’ve earned well rounded knowledge passion new world go always cool moving paddle pong lighting combos street fighter always revolved around programmer’s functional guess something behave fun programmers aren’t always gifted programming often see google “epic game fails” see glitches physics sometimes even experienced human players regardless new talent teach computer play video games understand language even identify people things tip iceberg new skill comes old concept recently got processing power exist outside theory i’m talking machine learning don’t need come advanced algorithms anymore teach computer come advanced algorithm something like even work algorithm isn’t really written much sort bred i’m using breeding analogy watch short video gives excellent commentary animations high level concept creating wow right that’s crazy process can’t even understand algorithm it’s done one great visual written beat mario games human understand play side scroller identifying predictive strategy resulting insane impressed there’s something amazing idea right problem don’t know machine learning don’t know hook video games fortunately elon musk already provided non profit company latter yes dozen lines code hook want countless games tasks two good answers care firstly machine learning ml making computers things we’ve never made computers want something new new world ml secondly don’t influence world world influence right significant companies investing ml we’re already seeing change world thought leaders warning can’t let new age algorithms exist outside public eye imagine corporate monoliths controlled internet don’t take arms science won’t think christian heilmann said best talk ml concept useful cool understand high level heck actually happening work want jump straight suggest skip section move next “how get started” section you’re motivated doer ml won’t need videos you’re still trying grasp could even thing following video perfect walking logic using classic ml problem handwriting pretty cool huh video shows layer gets simpler rather complicated like function chewing data smaller pieces end abstract concept get hands dirty interacting process site adam harley it’s cool watching data go trained model even watch neural network get trained one classic real world examples machine learning action iris data set 1936 presentation attended javafxpert’s overview machine learning learned use tool visualize adjustment back propagation weights neurons neural network get watch train neural model even you’re java buff presentation jim gives things machine learning pretty cool 1 5 hour introduction ml concepts includes info many examples concepts exciting ready einstein new era breakthroughs happening every day get started tons resources available i’ll recommending two approaches approach you’ll understand machine learning algorithms math know way sounds tough cool would really get details code stuff scratch want force ml hold deep conversations route recommend try brilliant org’s app always great science lover take artificial neural network course course time limits helps learn ml killing time line phone one costs money level 1 combine simultaneous enrollment andrew ng’s stanford course “machine learning 11 weeks” course jim weaver recommended video i’ve also course independently suggested jen looper everyone provides caveat course tough that’s show stopper others that’s you’re going put collect certificate saying course 100 free pay certificate want one two courses you’ll lot work everyone impressed make that’s simple make you’ll deep understanding implementation machine learning catapult successfully applying new world changing ways you’re interested writing algorithms want use create next breathtaking website app jump tensorflow crash course tensorflow de facto open source software library machine learning used countless ways even javascript here’s crash course plenty information available courses rankings found taking course style you’re still luck don’t learn nitty gritty ml order use today efficiently utilize ml service many ways tech giants trained models ready would still caution there’s guarantee data safe even offerings services ml quite attractive using ml service might best solution you’re excited able upload data amazon microsoft google like think services gateway drug advanced ml either way it’s good get started say thank aforementioned people videos inspiration get started though i’m still newb ml world i’m happy light path others embrace awe inspiring age find it’s imperative reach connect people take learning craft without friendly faces answers sounding boards anything hard able ask get response game changer add add people mentioned friendly people friendly advice helps see hope article inspired around learn ml quick cheer standing ovation clap show much enjoyed story software consultant adjunct professor published author award winning speaker mentor organizer immature nerd — lately full react native tech community publishes stories worth reading development design data science\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}